# Machine Learning Prediction Improvement Guide

## 1. Data Quality Issues (Most Common Cause of Poor Predictions)

### Data Preprocessing
- **Missing Values**: Handle NaN values properly
  ```python
  # Instead of dropping, try imputation
  from sklearn.impute import SimpleImputer, KNNImputer
  imputer = KNNImputer(n_neighbors=5)
  X_imputed = imputer.fit_transform(X)
  ```

- **Outlier Detection and Treatment**
  ```python
  from scipy import stats
  import numpy as np
  
  # Remove outliers using IQR method
  Q1 = data.quantile(0.25)
  Q3 = data.quantile(0.75)
  IQR = Q3 - Q1
  data_clean = data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]
  ```

- **Feature Scaling**
  ```python
  from sklearn.preprocessing import StandardScaler, RobustScaler
  
  # Use RobustScaler for data with outliers
  scaler = RobustScaler()
  X_scaled = scaler.fit_transform(X)
  ```

### Data Leakage Prevention
- Ensure no future information is used to predict past/present
- Remove features that are direct proxies of the target
- Check for temporal data leakage in time series

## 2. Feature Engineering Improvements

### Create Better Features
```python
# For time series data
data['rolling_mean_7'] = data['value'].rolling(window=7).mean()
data['rolling_std_7'] = data['value'].rolling(window=7).std()
data['lag_1'] = data['value'].shift(1)
data['lag_7'] = data['value'].shift(7)

# Interaction features
data['feature1_x_feature2'] = data['feature1'] * data['feature2']

# Polynomial features
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2, interaction_only=True)
X_poly = poly.fit_transform(X)
```

### Feature Selection
```python
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.ensemble import RandomForestRegressor

# Statistical feature selection
selector = SelectKBest(score_func=f_regression, k=10)
X_selected = selector.fit_transform(X, y)

# Recursive feature elimination
rf = RandomForestRegressor()
selector = RFE(rf, n_features_to_select=10)
X_selected = selector.fit_transform(X, y)
```

## 3. Model Selection and Hyperparameter Tuning

### Try Different Model Types
```python
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

models = {
    'rf': RandomForestRegressor(),
    'xgb': XGBRegressor(),
    'lgb': LGBMRegressor(),
    'ridge': Ridge(),
    'elastic': ElasticNet()
}
```

### Hyperparameter Optimization
```python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from scipy.stats import randint, uniform

# Random search (often better than grid search)
param_dist = {
    'n_estimators': randint(100, 1000),
    'max_depth': randint(3, 20),
    'learning_rate': uniform(0.01, 0.3),
    'subsample': uniform(0.6, 0.4)
}

random_search = RandomizedSearchCV(
    XGBRegressor(),
    param_distributions=param_dist,
    n_iter=100,
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1
)
```

## 4. Cross-Validation and Evaluation

### Proper Cross-Validation
```python
from sklearn.model_selection import TimeSeriesSplit, cross_val_score

# For time series data
tscv = TimeSeriesSplit(n_splits=5)
scores = cross_val_score(model, X, y, cv=tscv, scoring='neg_mean_squared_error')

# For regular data
from sklearn.model_selection import KFold
kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')
```

### Multiple Evaluation Metrics
```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def evaluate_model(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    print(f"MSE: {mse:.4f}")
    print(f"RMSE: {np.sqrt(mse):.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"RÂ²: {r2:.4f}")
    
    return {'mse': mse, 'rmse': np.sqrt(mse), 'mae': mae, 'r2': r2}
```

## 5. Ensemble Methods

### Simple Ensemble
```python
from sklearn.ensemble import VotingRegressor

# Create ensemble of different models
ensemble = VotingRegressor([
    ('rf', RandomForestRegressor(n_estimators=100)),
    ('xgb', XGBRegressor()),
    ('ridge', Ridge())
])

ensemble.fit(X_train, y_train)
predictions = ensemble.predict(X_test)
```

### Stacking
```python
from sklearn.ensemble import StackingRegressor

base_models = [
    ('rf', RandomForestRegressor()),
    ('xgb', XGBRegressor()),
    ('ridge', Ridge())
]

stacking_model = StackingRegressor(
    estimators=base_models,
    final_estimator=Ridge(),
    cv=5
)
```

## 6. Deep Learning Improvements (for Neural Networks)

### Better Architecture
```python
import torch
import torch.nn as nn

class ImprovedLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           dropout=dropout, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, 1)
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        # Use last output
        out = self.dropout(lstm_out[:, -1, :])
        return self.fc(out)
```

### Training Improvements
- Use learning rate scheduling
- Add early stopping
- Use gradient clipping
- Try different optimizers (Adam, AdamW, etc.)

## 7. Specific Fixes for Common Issues

### High Bias (Underfitting)
- Increase model complexity
- Add more features
- Reduce regularization
- Train longer

### High Variance (Overfitting)
- Add regularization (L1, L2, dropout)
- Reduce model complexity
- Get more training data
- Use cross-validation

### Data Distribution Issues
- Check for data shifts between train/test
- Ensure proper train/validation/test splits
- Use stratified sampling for imbalanced data

## 8. Debugging Checklist

1. **Data Sanity Checks**
   - Plot your data and look for obvious issues
   - Check distributions of features and target
   - Verify no data leakage

2. **Model Sanity Checks**
   - Start with a simple baseline model
   - Ensure your model can overfit a small dataset
   - Check learning curves

3. **Prediction Analysis**
   - Plot predictions vs actual values
   - Analyze residuals
   - Look for patterns in errors

## Next Steps

To give you more specific advice, please share:
1. Your current model architecture
2. Dataset characteristics (size, features, target variable)
3. Current performance metrics
4. The type of problem (regression, classification, time series)

This will help identify the most likely causes of poor performance in your specific case.